{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "1. Load training corpus\n",
    "2. Tokenize\n",
    "   1. Normalize\n",
    "3. Create n-grams\n",
    "4. Count n-grams as history + continuation\n",
    "5. Estimate probabilities\n",
    "6. Sample text from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "# Read txt files in data/gutenberg and put into a df\n",
    "def read_txt_files():\n",
    "    # Get all txt files in data/gutenberg\n",
    "    \n",
    "    ################################################ \n",
    "    # Data folder path\n",
    "    path = pathlib.Path(\"../data/gutenberg\")\n",
    "    ################################################\n",
    "    \n",
    "    txt_files = path.glob(\"*.txt\")\n",
    "\n",
    "    # Read each txt file and put into a df\n",
    "    data = []\n",
    "    for txt_file in txt_files:\n",
    "        with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            # Find title, author, and language\n",
    "            title = None\n",
    "            author = None\n",
    "            language = None\n",
    "            start = 0\n",
    "            end = len(lines)\n",
    "            for i, line in enumerate(lines[:100]):\n",
    "                if \"Title: \" in line:\n",
    "                    title = line.split(\"Title: \")[1].strip()\n",
    "                if \"Author: \" in line:\n",
    "                    author = line.split(\"Author: \")[1].strip()\n",
    "                if \"Language: \" in line:\n",
    "                    language = line.split(\"Language: \")[1].strip()\n",
    "                \n",
    "                if (line.__contains__(\"*** START OF THE PROJECT GUTENBERG EBOOK\")):\n",
    "                    start = i\n",
    "                    break\n",
    "                    \n",
    "            for i, line in enumerate(lines[-1000:]):\n",
    "                if (line.__contains__(\"*** END OF THE PROJECT GUTENBERG EBOOK\")):\n",
    "                    end = i\n",
    "                    break\n",
    "                \n",
    "                \n",
    "            content = \"\".join(lines[start+1:end-1])\n",
    "            data.append({\"title\": title, \"author\": author, \"language\": language, \"content\": content})\n",
    "    df = pd.DataFrame(data)\n",
    "    df.set_index(\"title\", inplace=True)\n",
    "    return df\n",
    "\n",
    "df = read_txt_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rabjho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.downloader\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Tokenization\n",
    "df[\"tokens\"] = df[\"content\"].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "df[\"ngrams\"] = df[\"tokens\"].apply(lambda x: list(ngrams(x, 3)))\n",
    "\n",
    "def create_ngram_model(tokens, n):\n",
    "    ngram_model = defaultdict(Counter)\n",
    "    for ngram in nltk.ngrams(tokens, n):\n",
    "        history = ngram[:-1]\n",
    "        continuation = ngram[-1]\n",
    "        ngram_model[history][continuation] += 1\n",
    "    return ngram_model\n",
    "\n",
    "df[\"raw_model\"] = df[\"tokens\"].apply(lambda tokens: create_ngram_model(tokens, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize probabilities by deviding history+continuation count by history count\n",
    "def normalize(counter):\n",
    "    total = float(sum(counter.values()))\n",
    "    return [(key, val/total) for key, val in counter.items()]\n",
    "\n",
    "df[\"model\"] = df[\"raw_model\"].apply(lambda x: {history: normalize(continuations) for history, continuations in x.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rabjho\\AppData\\Local\\Temp\\ipykernel_2836\\1689488980.py:18: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  seed = random.choice(list(df[\"raw_model\"][document].keys()))\n",
      "C:\\Users\\Rabjho\\AppData\\Local\\Temp\\ipykernel_2836\\1689488980.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  generate_text(df[\"model\"][document], seed, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'go in sailor-hats instead . Every one takes them for English , Miss Honeychurch will be ready , ” his advice concluded . “ Don ’ t think much harm would have come of accepting. ” “ Mother wouldn ’ t you feel , too , to fling wide the windows , pinching the fingers in unfamiliar fastenings , to open the eyes upon a bright bare room , sighed heavily according to her cousin had permitted it . “ Charlotte , being poor. ” Fortunately one of them—one of the narrow world at Tunbridge Wells , she would be hard indeed'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_text(model, seed, n):\n",
    "    history = tuple(seed)\n",
    "    text = []\n",
    "    for i in range(n):\n",
    "        history = tuple(history)\n",
    "        if history not in model:\n",
    "            break\n",
    "        possibilities = model[history]\n",
    "        continuation = random.choices([x[0] for x in possibilities], [x[1] for x in possibilities])[0]\n",
    "        text.append(continuation)\n",
    "        history = list(history[1:]) + [continuation]\n",
    "    return \" \".join(seed+tuple(text))\n",
    "\n",
    "document = 0\n",
    "# Choose seed from likely ngrams in col raw_model\n",
    "seed = random.choice(list(df[\"raw_model\"][document].keys()))\n",
    "generate_text(df[\"model\"][document], seed, 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
